"""
Korean Excel Template Crawler Service
í•œêµ­ì–´ ì—‘ì…€ í…œí”Œë¦¿ í¬ë¡¤ë§ ë° ìë™ ë¶„ë¥˜ ì„œë¹„ìŠ¤
"""
import asyncio
import aiohttp
import aiofiles
import logging
import os
import re
import json
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from datetime import datetime
from urllib.parse import urljoin, urlparse
from dataclasses import dataclass, asdict
import hashlib

from bs4 import BeautifulSoup
from ..models.template_metadata import EnhancedTemplateMetadata, TemplateCategory, TemplateComplexity

logger = logging.getLogger(__name__)


@dataclass
class CrawledTemplate:
    """í¬ë¡¤ë§ëœ í…œí”Œë¦¿ ì •ë³´"""
    title: str
    description: str
    download_url: str
    category_major: str
    category_minor: str
    file_name: str
    file_size: Optional[int] = None
    related_files: List[str] = None
    source_url: str = ""
    crawled_at: str = ""
    local_path: str = ""
    
    def __post_init__(self):
        if self.related_files is None:
            self.related_files = []
        if not self.crawled_at:
            self.crawled_at = datetime.now().isoformat()


class KoreanTemplateCrawler:
    """í•œêµ­ì–´ ì—‘ì…€ í…œí”Œë¦¿ í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        self.base_url = "https://excel.yesform.com"
        self.login_url = "https://www.yesform.com/z_n/member/login.php"
        self.start_url = "https://excel.yesform.com/docs/formList.php?division=A12"
        
        # ì‹¤ì œ ì¹´í…Œê³ ë¦¬ URL ë§¤í•‘
        self.category_urls = {
            "ì—…ë¬´í”„ë¡œê·¸ë¨": "https://excel.yesform.com/docs/formList.php?division=A12B11",
            "ì—…ë¬´í…œí”Œë¦¿": "https://excel.yesform.com/docs/formList.php?division=A12B12", 
            "ì°¨íŠ¸/ëŒ€ì‹œë³´ë“œ": "https://excel.yesform.com/docs/formList.php?division=A12B13"
        }
        
        # ë¡œê·¸ì¸ ì •ë³´
        self.username = "j1global"
        self.password = "wpdldnjs11!"
        
        # ì €ì¥ ê²½ë¡œ
        self.download_dir = Path("downloads/korean_templates")
        self.metadata_dir = Path("metadata/korean_templates")
        self.reports_dir = Path("reports/korean_templates")
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        for directory in [self.download_dir, self.metadata_dir, self.reports_dir]:
            directory.mkdir(parents=True, exist_ok=True)
        
        # ì„¸ì…˜ ë° ìƒíƒœ
        self.session = None
        self.crawled_templates = []
        self.failed_downloads = []
        self.categories_mapping = {
            "ì—‘ì…€í”„ë¡œê·¸ë¨": TemplateCategory.GENERAL,
            "ì—‘ì…€í…œí”Œë¦¿": TemplateCategory.GENERAL,
            "ì°¨íŠ¸/ëŒ€ì‹œë³´ë“œ": TemplateCategory.GENERAL
        }
        
        # ì§„í–‰ ìƒí™© ì¶”ì 
        self.progress = {
            "total_categories": 0,
            "processed_categories": 0,
            "total_templates": 0,
            "downloaded_templates": 0,
            "failed_downloads": 0,
            "start_time": None,
            "current_status": "ì¤€ë¹„ ì¤‘"
        }
    
    async def start_crawling(self) -> Dict[str, Any]:
        """í¬ë¡¤ë§ ì‹œì‘"""
        logger.info("ğŸš€ í•œêµ­ì–´ í…œí”Œë¦¿ í¬ë¡¤ë§ ì‹œì‘")
        self.progress["start_time"] = datetime.now()
        self.progress["current_status"] = "ë¡œê·¸ì¸ ì¤‘"
        
        try:
            # ì„¸ì…˜ ì´ˆê¸°í™” ë° ë¡œê·¸ì¸
            await self._initialize_session()
            await self._login()
            
            # ëŒ€ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘
            major_categories = await self._get_major_categories()
            self.progress["total_categories"] = len(major_categories)
            
            logger.info(f"ğŸ“‚ ë°œê²¬ëœ ëŒ€ë¶„ë¥˜: {len(major_categories)}ê°œ")
            
            # ê° ëŒ€ë¶„ë¥˜ë³„ë¡œ í¬ë¡¤ë§
            for major_category in major_categories:
                self.progress["current_status"] = f"ì²˜ë¦¬ ì¤‘: {major_category['name']}"
                await self._crawl_major_category(major_category)
                self.progress["processed_categories"] += 1
            
            # ê²°ê³¼ ì •ë¦¬ ë° ì €ì¥
            await self._save_crawling_results()
            await self._generate_summary_report()
            
            self.progress["current_status"] = "ì™„ë£Œ"
            
            return {
                "status": "success",
                "message": "í¬ë¡¤ë§ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤",
                "summary": {
                    "total_templates": len(self.crawled_templates),
                    "successful_downloads": self.progress["downloaded_templates"],
                    "failed_downloads": self.progress["failed_downloads"],
                    "categories_processed": self.progress["processed_categories"],
                    "duration": str(datetime.now() - self.progress["start_time"])
                }
            }
        
        except Exception as e:
            logger.error(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            self.progress["current_status"] = f"ì˜¤ë¥˜: {str(e)}"
            raise
        
        finally:
            if self.session:
                await self.session.close()
    
    async def _initialize_session(self):
        """HTTP ì„¸ì…˜ ì´ˆê¸°í™”"""
        timeout = aiohttp.ClientTimeout(total=30)
        self.session = aiohttp.ClientSession(
            timeout=timeout,
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
        )
    
    async def _login(self):
        """ì‚¬ì´íŠ¸ ë¡œê·¸ì¸ - ì‚¬ìš©ì ì œê³µ ì¿ í‚¤ ìš°ì„  ì‚¬ìš©"""
        logger.info("ğŸ” ë¡œê·¸ì¸ ì‹œë„ ì¤‘...")
        
        # ì™¸ë¶€ ì¿ í‚¤ íŒŒì¼ í™•ì¸
        cookie_file = Path("user_cookies.txt")
        if cookie_file.exists():
            return await self._load_user_cookies()
        
        # ìë™ ë¡œê·¸ì¸ ì‹œë„
        return await self._auto_login()
    
    async def _load_user_cookies(self):
        """ì‚¬ìš©ìê°€ ì œê³µí•œ ì¿ í‚¤ ë¡œë“œ"""
        cookie_file = Path("user_cookies.txt")
        logger.info("ğŸª ì‚¬ìš©ì ì œê³µ ì¿ í‚¤ ë¡œë“œ ì¤‘...")
        
        try:
            async with aiofiles.open(cookie_file, 'r', encoding='utf-8') as f:
                cookie_data = await f.read()
            
            # ì¿ í‚¤ í˜•ì‹ íŒŒì‹± (name=value; name2=value2 í˜•ì‹)
            cookies_applied = 0
            from urllib.parse import urlparse
            
            for cookie_line in cookie_data.strip().split('\n'):
                if '=' in cookie_line and not cookie_line.strip().startswith('#'):
                    for cookie_pair in cookie_line.split(';'):
                        cookie_pair = cookie_pair.strip()
                        if '=' in cookie_pair:
                            name, value = cookie_pair.split('=', 1)
                            
                            # ìˆ˜ë™ìœ¼ë¡œ ì¿ í‚¤ í—¤ë” ì„¤ì •
                            cookie_header = f"{name.strip()}={value.strip()}"
                            
                            # ì„¸ì…˜ì˜ ê¸°ë³¸ í—¤ë”ì— ì¿ í‚¤ ì¶”ê°€
                            if 'Cookie' not in self.session.headers:
                                self.session.headers['Cookie'] = cookie_header
                            else:
                                self.session.headers['Cookie'] += f"; {cookie_header}"
                            
                            cookies_applied += 1
                            logger.info(f"ì¿ í‚¤ ì„¤ì •: {name.strip()}={value.strip()[:10]}...")
            
            logger.info(f"ì´ {cookies_applied}ê°œ ì¿ í‚¤ ì ìš©ë¨")
            
            # ì¿ í‚¤ ì ìš© í™•ì¸ì„ ìœ„í•œ í…ŒìŠ¤íŠ¸ ìš”ì²­
            async with self.session.get(self.base_url) as response:
                logger.info(f"ì¿ í‚¤ í…ŒìŠ¤íŠ¸ ì‘ë‹µ: {response.status}")
                # ì‘ë‹µ ë‚´ìš© í™•ì¸
                try:
                    content = await response.text(encoding='utf-8')
                except UnicodeDecodeError:
                    content_bytes = await response.read()
                    try:
                        content = content_bytes.decode('euc-kr')
                    except UnicodeDecodeError:
                        content = content_bytes.decode('cp949', errors='ignore')
                
                # ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸ (ë¡œê·¸ì•„ì›ƒ ë§í¬ê°€ ìˆëŠ”ì§€ í™•ì¸)
                is_logged_in = 'ë¡œê·¸ì•„ì›ƒ' in content or 'logout' in content.lower()
                
                if is_logged_in:
                    logger.info("âœ… ì‚¬ìš©ì ì¿ í‚¤ ì ìš© ì„±ê³µ - ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸ë¨")
                    return True
                else:
                    logger.warning(f"âš ï¸  ì¿ í‚¤ëŠ” ì ìš©ë˜ì—ˆì§€ë§Œ ë¡œê·¸ì¸ ìƒíƒœ ë¶ˆí™•ì‹¤")
                    # ì¼ë‹¨ ê³„ì† ì§„í–‰í•´ë³´ê¸°
                    return True
                    
        except Exception as e:
            logger.error(f"âŒ ì¿ í‚¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    async def _auto_login(self):
        """ìë™ ë¡œê·¸ì¸ ì‹œë„"""
        try:
            # ë¨¼ì € ë©”ì¸ ì‚¬ì´íŠ¸ì— ì ‘ì†í•˜ì—¬ ì„¸ì…˜ ìƒì„±
            async with self.session.get(self.base_url) as response:
                logger.info(f"ë©”ì¸ ì‚¬ì´íŠ¸ ì ‘ì†: {response.status}")
            
            # ë¡œê·¸ì¸ í˜ì´ì§€ ì ‘ì†
            async with self.session.get(self.login_url) as response:
                if response.status != 200:
                    raise Exception(f"ë¡œê·¸ì¸ í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨: {response.status}")
                
                # í•œêµ­ì–´ ì‚¬ì´íŠ¸ ì¸ì½”ë”© ì²˜ë¦¬
                try:
                    login_page = await response.text(encoding='utf-8')
                except UnicodeDecodeError:
                    # ì¸ì½”ë”© ì˜¤ë¥˜ ì‹œ ëŒ€ì²´ ë°©ë²• ì‚¬ìš©
                    content = await response.read()
                    try:
                        login_page = content.decode('euc-kr')
                    except UnicodeDecodeError:
                        login_page = content.decode('cp949', errors='ignore')
                
                soup = BeautifulSoup(login_page, 'html.parser')
                logger.info(f"ë¡œê·¸ì¸ í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ: {len(login_page)} ë°”ì´íŠ¸")
                
                # ë¡œê·¸ì¸ í¼ ì°¾ê¸°
                login_form = soup.find('form', {'name': 'login'}) or soup.find('form')
                if login_form:
                    action = login_form.get('action', self.login_url)
                    if not action.startswith('http'):
                        action = f"https://www.yesform.com{action}" if action.startswith('/') else f"https://www.yesform.com/z_n/member/{action}"
                    logger.info(f"ë¡œê·¸ì¸ ì•¡ì…˜ URL: {action}")
                else:
                    action = "https://www.yesform.com/z_n/member/login_check.php"
                    logger.info("ê¸°ë³¸ ë¡œê·¸ì¸ ì•¡ì…˜ URL ì‚¬ìš©")
                
                # CSRF í† í°ì´ë‚˜ íˆë“  í•„ë“œ í™•ì¸
                hidden_inputs = soup.find_all('input', type='hidden')
                login_data = {
                    'pId': self.username,
                    'pPwd': self.password,
                }
                
                # íˆë“  í•„ë“œ ì¶”ê°€
                for hidden in hidden_inputs:
                    if hidden.get('name'):
                        login_data[hidden.get('name')] = hidden.get('value', '')
                        logger.info(f"íˆë“  í•„ë“œ ì¶”ê°€: {hidden.get('name')} = {hidden.get('value', '')}")
            
            # ë¡œê·¸ì¸ ìš”ì²­
            logger.info(f"ë¡œê·¸ì¸ ë°ì´í„°: {login_data}")
            async with self.session.post(action, data=login_data) as response:
                logger.info(f"ë¡œê·¸ì¸ ì‘ë‹µ ìƒíƒœ: {response.status}")
                logger.info(f"ë¡œê·¸ì¸ ì‘ë‹µ URL: {response.url}")
                
                if response.status != 200:
                    raise Exception(f"ë¡œê·¸ì¸ ìš”ì²­ ì‹¤íŒ¨: {response.status}")
                
                # ì‘ë‹µ ë‚´ìš© í™•ì¸
                try:
                    response_text = await response.text(encoding='utf-8')
                except UnicodeDecodeError:
                    content = await response.read()
                    try:
                        response_text = content.decode('euc-kr')
                    except UnicodeDecodeError:
                        response_text = content.decode('cp949', errors='ignore')
                
                # ë¡œê·¸ì¸ ì„±ê³µ ì—¬ë¶€ í™•ì¸
                if 'ë¡œê·¸ì¸' in response_text and 'login' in str(response.url).lower():
                    logger.warning(f"ë¡œê·¸ì¸ ì‹¤íŒ¨ ê°€ëŠ¥ì„± - URL: {response.url}")
                    logger.warning(f"ì‘ë‹µ ë‚´ìš© ì¼ë¶€: {response_text[:200]}")
                    # ì¼ë‹¨ ê³„ì† ì§„í–‰í•´ë³´ê¸° - ì‹¤ì œ í˜ì´ì§€ ì ‘ê·¼ì—ì„œ íŒë‹¨
                else:
                    logger.info("ë¡œê·¸ì¸ ì‘ë‹µ ì •ìƒ - ë¡œê·¸ì¸ ì„±ê³µ ê°€ëŠ¥ì„± ë†’ìŒ")
                
                logger.info("âœ… ë¡œê·¸ì¸ ì‹œë„ ì™„ë£Œ")
                return True
        
        except Exception as e:
            logger.error(f"âŒ ë¡œê·¸ì¸ ì‹¤íŒ¨: {e}")
            raise
    
    async def _get_major_categories(self) -> List[Dict[str, str]]:
        """ëŒ€ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘"""
        logger.info("ğŸ“‚ ëŒ€ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ì¤‘...")
        
        # í•˜ë“œì½”ë”©ëœ ì¹´í…Œê³ ë¦¬ URL ì‚¬ìš© (ì‹¤ì œ ì‚¬ì´íŠ¸ êµ¬ì¡° ê¸°ë°˜)
        categories = []
        for name, url in self.category_urls.items():
            categories.append({
                'name': name,
                'url': url
            })
            
        logger.info(f"âœ… ëŒ€ë¶„ë¥˜ {len(categories)}ê°œ ìˆ˜ì§‘: {[cat['name'] for cat in categories]}")
        return categories
    
    async def _crawl_major_category(self, major_category: Dict[str, str]):
        """ëŒ€ë¶„ë¥˜ë³„ í¬ë¡¤ë§"""
        logger.info(f"ğŸ” '{major_category['name']}' ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹œì‘")
        
        try:
            # ëŒ€ë¶„ë¥˜ í˜ì´ì§€ ì ‘ì†
            async with self.session.get(major_category['url']) as response:
                if response.status != 200:
                    logger.error(f"ëŒ€ë¶„ë¥˜ í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨: {response.status}")
                    return
                
                # í•œêµ­ì–´ ì‚¬ì´íŠ¸ ì¸ì½”ë”© ì²˜ë¦¬
                try:
                    page_content = await response.text(encoding='utf-8')
                except UnicodeDecodeError:
                    content = await response.read()
                    try:
                        page_content = content.decode('euc-kr')
                    except UnicodeDecodeError:
                        page_content = content.decode('cp949', errors='ignore')
                soup = BeautifulSoup(page_content, 'html.parser')
                
                # ì¤‘ë¶„ë¥˜ ìˆ˜ì§‘ ë° ì²˜ë¦¬
                minor_categories = await self._get_minor_categories(soup, major_category['url'])
                logger.info(f"ğŸ“ '{major_category['name']}'ì—ì„œ ì¤‘ë¶„ë¥˜ {len(minor_categories)}ê°œ ë°œê²¬")
                
                if not minor_categories:
                    # ì¤‘ë¶„ë¥˜ê°€ ì—†ìœ¼ë©´ ì§ì ‘ í…œí”Œë¦¿ ìˆ˜ì§‘
                    template_links = await self._get_template_links(soup)
                    logger.info(f"ğŸ“‹ '{major_category['name']}'ì—ì„œ í…œí”Œë¦¿ {len(template_links)}ê°œ ë°œê²¬")
                    
                    for template_link in template_links:
                        await self._process_template(major_category['name'], "ì¼ë°˜", template_link)
                else:
                    # ê° ì¤‘ë¶„ë¥˜ë³„ë¡œ í…œí”Œë¦¿ ìˆ˜ì§‘
                    for minor_category in minor_categories:
                        await self._crawl_minor_category_with_pagination(major_category['name'], minor_category)
        
        except Exception as e:
            logger.error(f"âŒ '{major_category['name']}' í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    
    async def _get_minor_categories(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, str]]:
        """ì¤‘ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘"""
        minor_categories = []
        
        # í•„í„° ì„¹ì…˜ì—ì„œ ì¤‘ë¶„ë¥˜ ì°¾ê¸°
        filter_section = soup.find('div', class_='filter__format')
        if not filter_section:
            logger.warning("ì¤‘ë¶„ë¥˜ í•„í„° ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return minor_categories
        
        # input ìš”ì†Œë“¤ì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
        for input_elem in filter_section.find_all('input'):
            value = input_elem.get('value')
            next_label = input_elem.find_next_sibling('label')
            
            if value and next_label:
                category_name = next_label.get_text(strip=True)
                if category_name and value != 'all':  # 'all' ì œì™¸
                    minor_categories.append({
                        'name': category_name,
                        'value': value,
                        'base_url': base_url
                    })
        
        return minor_categories
    
    async def _crawl_minor_category(self, major_name: str, minor_category: Dict[str, str]):
        """ì¤‘ë¶„ë¥˜ë³„ í…œí”Œë¦¿ í¬ë¡¤ë§"""
        logger.info(f"ğŸ“„ ì¤‘ë¶„ë¥˜ '{minor_category['name']}' í…œí”Œë¦¿ ìˆ˜ì§‘ ì¤‘...")
        
        try:
            # ì¤‘ë¶„ë¥˜ í•„í„° ì ìš©ëœ URL ìƒì„±
            category_url = f"{minor_category['base_url']}&format={minor_category['value']}"
            
            async with self.session.get(category_url) as response:
                if response.status != 200:
                    logger.error(f"ì¤‘ë¶„ë¥˜ í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨: {response.status}")
                    return
                
                # í•œêµ­ì–´ ì‚¬ì´íŠ¸ ì¸ì½”ë”© ì²˜ë¦¬
                try:
                    page_content = await response.text(encoding='utf-8')
                except UnicodeDecodeError:
                    content = await response.read()
                    try:
                        page_content = content.decode('euc-kr')
                    except UnicodeDecodeError:
                        page_content = content.decode('cp949', errors='ignore')
                soup = BeautifulSoup(page_content, 'html.parser')
                
                # í…œí”Œë¦¿ ëª©ë¡ ìˆ˜ì§‘
                template_links = await self._get_template_links(soup)
                
                logger.info(f"ğŸ“‹ '{minor_category['name']}'ì—ì„œ í…œí”Œë¦¿ {len(template_links)}ê°œ ë°œê²¬")
                
                # ê° í…œí”Œë¦¿ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ë° ë‹¤ìš´ë¡œë“œ
                for template_link in template_links:
                    await self._process_template(major_name, minor_category['name'], template_link)
        
        except Exception as e:
            logger.error(f"âŒ ì¤‘ë¶„ë¥˜ '{minor_category['name']}' ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    async def _get_minor_categories(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, str]]:
        """ì¤‘ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘"""
        minor_categories = []
        
        try:
            # í•„í„° ì„¹ì…˜ì—ì„œ ì¤‘ë¶„ë¥˜ ì°¾ê¸°
            filter_sections = soup.find_all('div', class_='filter__item')
            
            for section in filter_sections:
                section_title = section.find('h3')
                if not section_title or 'ì¹´í…Œê³ ë¦¬' not in section_title.get_text():
                    continue
                
                # ì¹´í…Œê³ ë¦¬ ì˜µì…˜ë“¤ ì°¾ê¸°
                for label in section.find_all('label'):
                    input_elem = label.find('input')
                    if input_elem and input_elem.get('value') and input_elem.get('value') != 'all':
                        category_name = label.get_text(strip=True)
                        category_value = input_elem.get('value')
                        
                        minor_categories.append({
                            'name': category_name,
                            'value': category_value,
                            'base_url': base_url
                        })
            
            return minor_categories
        except Exception as e:
            logger.error(f"ì¤‘ë¶„ë¥˜ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    async def _crawl_minor_category_with_pagination(self, major_name: str, minor_category: Dict[str, str]):
        """ì¤‘ë¶„ë¥˜ë³„ í˜ì´ì§€ë„¤ì´ì…˜ í¬í•¨ í¬ë¡¤ë§"""
        logger.info(f"ğŸ” ì¤‘ë¶„ë¥˜ '{minor_category['name']}' í˜ì´ì§€ë„¤ì´ì…˜ í¬ë¡¤ë§ ì‹œì‘")
        
        page = 1
        while True:
            try:
                # í˜ì´ì§€ë³„ URL ìƒì„±
                if '?' in minor_category['base_url']:
                    page_url = f"{minor_category['base_url']}&category={minor_category['value']}&page={page}"
                else:
                    page_url = f"{minor_category['base_url']}?category={minor_category['value']}&page={page}"
                
                async with self.session.get(page_url) as response:
                    if response.status != 200:
                        break
                    
                    # í•œêµ­ì–´ ì‚¬ì´íŠ¸ ì¸ì½”ë”© ì²˜ë¦¬
                    try:
                        page_content = await response.text(encoding='utf-8')
                    except UnicodeDecodeError:
                        content = await response.read()
                        try:
                            page_content = content.decode('euc-kr')
                        except UnicodeDecodeError:
                            page_content = content.decode('cp949', errors='ignore')
                    
                    soup = BeautifulSoup(page_content, 'html.parser')
                    
                    # í…œí”Œë¦¿ ë§í¬ ìˆ˜ì§‘
                    template_links = await self._get_template_links(soup)
                    
                    if not template_links:
                        logger.info(f"ğŸ“„ '{minor_category['name']}' í˜ì´ì§€ {page}: ë” ì´ìƒ í…œí”Œë¦¿ì´ ì—†ìŒ")
                        break
                    
                    logger.info(f"ğŸ“„ '{minor_category['name']}' í˜ì´ì§€ {page}: {len(template_links)}ê°œ í…œí”Œë¦¿ ë°œê²¬")
                    
                    # ê° í…œí”Œë¦¿ ì²˜ë¦¬
                    for template_link in template_links:
                        await self._process_template(major_name, minor_category['name'], template_link)
                    
                    page += 1
                    
                    # í˜ì´ì§€ ì œí•œ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
                    if page > 50:
                        logger.warning(f"âš ï¸  í˜ì´ì§€ ì œí•œ ë„ë‹¬: {minor_category['name']}")
                        break
                    
                    # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
                    await asyncio.sleep(1)
                    
            except Exception as e:
                logger.error(f"âŒ ì¤‘ë¶„ë¥˜ '{minor_category['name']}' í˜ì´ì§€ {page} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                break
    
    async def _get_template_links(self, soup: BeautifulSoup) -> List[str]:
        """í…œí”Œë¦¿ ë§í¬ ìˆ˜ì§‘"""
        template_links = []
        
        # ë¦¬ìŠ¤íŠ¸ ì„¹ì…˜ì—ì„œ í…œí”Œë¦¿ ë§í¬ ì°¾ê¸° (.ListSection í´ë˜ìŠ¤ ì‚¬ìš©)
        list_section = soup.find('div', class_='ListSection')
        if not list_section:
            # ëŒ€ì²´ ì„ íƒìë“¤ ì‹œë„
            list_section = soup.find('div', id='ListSection') or soup.find('section', class_='list-section')
            if not list_section:
                logger.warning("í…œí”Œë¦¿ ë¦¬ìŠ¤íŠ¸ ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return template_links
        
        # xlsë¡œ ì‹œì‘í•˜ëŠ” í…œí”Œë¦¿ ë§í¬ ìˆ˜ì§‘
        for link in list_section.find_all('a', href=True):
            href = link.get('href')
            if href and (href.startswith('/xls/') or 'xls' in href):
                full_url = urljoin(self.base_url, href)
                template_links.append(full_url)
                logger.info(f"í…œí”Œë¦¿ ë§í¬ ë°œê²¬: {href}")
        
        return list(set(template_links))  # ì¤‘ë³µ ì œê±°
    
    async def _process_template(self, major_category: str, minor_category: str, template_url: str):
        """ê°œë³„ í…œí”Œë¦¿ ì²˜ë¦¬"""
        try:
            # ìƒì„¸ í˜ì´ì§€ ì ‘ì†
            async with self.session.get(template_url) as response:
                if response.status != 200:
                    logger.error(f"í…œí”Œë¦¿ ìƒì„¸ í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨: {response.status}")
                    return
                
                # í•œêµ­ì–´ ì‚¬ì´íŠ¸ ì¸ì½”ë”© ì²˜ë¦¬
                try:
                    page_content = await response.text(encoding='utf-8')
                except UnicodeDecodeError:
                    content = await response.read()
                    try:
                        page_content = content.decode('euc-kr')
                    except UnicodeDecodeError:
                        page_content = content.decode('cp949', errors='ignore')
                soup = BeautifulSoup(page_content, 'html.parser')
                
                # í…œí”Œë¦¿ ì •ë³´ ì¶”ì¶œ
                template_info = await self._extract_template_info(soup, template_url)
                if not template_info:
                    return
                
                # ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¶”ê°€
                template_info['category_major'] = major_category
                template_info['category_minor'] = minor_category
                
                # í…œí”Œë¦¿ ë‹¤ìš´ë¡œë“œ
                downloaded_path = await self._download_template(template_info)
                if downloaded_path:
                    template_info['local_path'] = str(downloaded_path)
                    self.progress["downloaded_templates"] += 1
                else:
                    self.progress["failed_downloads"] += 1
                    self.failed_downloads.append(template_info)
                
                # ì—°ê´€ íŒŒì¼ ì²˜ë¦¬
                related_files = await self._get_related_files(soup)
                template_info['related_files'] = related_files
                
                # í¬ë¡¤ë§ëœ í…œí”Œë¦¿ ëª©ë¡ì— ì¶”ê°€
                crawled_template = CrawledTemplate(**template_info)
                self.crawled_templates.append(crawled_template)
                
                logger.info(f"âœ… í…œí”Œë¦¿ ì²˜ë¦¬ ì™„ë£Œ: {template_info['title']}")
        
        except Exception as e:
            logger.error(f"âŒ í…œí”Œë¦¿ ì²˜ë¦¬ ì‹¤íŒ¨ ({template_url}): {e}")
    
    async def _extract_template_info(self, soup: BeautifulSoup, source_url: str) -> Optional[Dict[str, str]]:
        """í…œí”Œë¦¿ ì •ë³´ ì¶”ì¶œ"""
        try:
            # ì œëª© ì¶”ì¶œ
            title_elem = soup.find('h1')
            if not title_elem:
                title_elem = soup.find(class_='title')
                if title_elem:
                    title_elem = title_elem.find('h1')
            
            title = title_elem.get_text(strip=True) if title_elem else "ì œëª© ì—†ìŒ"
            
            # ì„¤ëª… ì¶”ì¶œ
            description_elem = soup.find('div', id='description')
            description = description_elem.get_text(strip=True) if description_elem else "ì„¤ëª… ì—†ìŒ"
            
            # ë‹¤ìš´ë¡œë“œ URL ì¶”ì¶œ (ì‚¬ìš©ì ì œê³µ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •)
            download_url = None
            
            # format-List ì„¹ì…˜ì—ì„œ Excel ë‹¤ìš´ë¡œë“œ ë§í¬ ì°¾ê¸°
            format_list = soup.find('ul', class_='format-List')
            if format_list:
                # Excel ì•„ì´ì½˜ì´ ìˆëŠ” ë§í¬ ì°¾ê¸°
                excel_links = format_list.find_all('a', href=True)
                for link in excel_links:
                    href = link.get('href')
                    # dw_form.phpê°€ í¬í•¨ë˜ê³  sample íŒŒë¼ë¯¸í„°ê°€ ì—†ëŠ” ë§í¬ (ë©”ì¸ íŒŒì¼)
                    if href and 'dw_form.php' in href and 'sample=' not in href:
                        download_url = href
                        break
            
            # ëŒ€ì²´ ë°©ë²•: img alt ì†ì„±ìœ¼ë¡œ ì°¾ê¸°
            if not download_url:
                download_elem = soup.find('img', alt='Microsoft Excel (xlsx)')
                if download_elem:
                    download_link = download_elem.find_parent('a')
                    download_url = download_link.get('href') if download_link else None
            
            if not download_url:
                logger.warning(f"ë‹¤ìš´ë¡œë“œ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {title}")
                return None
            
            # íŒŒì¼ëª… ìƒì„± (ì œëª© ê¸°ë°˜)
            safe_title = re.sub(r'[^\w\-_\.]', '_', title)
            file_name = f"{safe_title}.xlsx"
            
            # ë‹¤ìš´ë¡œë“œ URLì„ ì˜¬ë°”ë¥¸ ë„ë©”ì¸ìœ¼ë¡œ ìƒì„±
            if download_url.startswith('/'):
                full_download_url = f"https://www.yesform.com{download_url}"
            elif download_url.startswith('http'):
                full_download_url = download_url
            else:
                full_download_url = f"https://www.yesform.com/z_n/forms/{download_url}"
            
            logger.info(f"ìƒì„±ëœ ë‹¤ìš´ë¡œë“œ URL: {full_download_url}")
            
            return {
                'title': title,
                'description': description,
                'download_url': full_download_url,
                'file_name': file_name,
                'source_url': source_url
            }
        
        except Exception as e:
            logger.error(f"í…œí”Œë¦¿ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    async def _download_template(self, template_info: Dict[str, str]) -> Optional[Path]:
        """í…œí”Œë¦¿ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        try:
            download_url = template_info['download_url']
            file_name = template_info['file_name']
            
            # ì¹´í…Œê³ ë¦¬ë³„ ë””ë ‰í† ë¦¬ ìƒì„±
            category_dir = self.download_dir / template_info['category_major'] / template_info['category_minor']
            category_dir.mkdir(parents=True, exist_ok=True)
            
            # íŒŒì¼ ê²½ë¡œ
            file_path = category_dir / file_name
            
            # ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì´ ìˆìœ¼ë©´ ìŠ¤í‚µ
            if file_path.exists():
                logger.info(f"â­ï¸  ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼: {file_name}")
                return file_path
            
            # ë‹¤ìš´ë¡œë“œ í—¤ë” ì„¤ì • (ë¸Œë¼ìš°ì € ì‹œë®¬ë ˆì´ì…˜)
            download_headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
            
            logger.info(f"ë‹¤ìš´ë¡œë“œ ì‹œì‘: {download_url}")
            
            # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            async with self.session.get(download_url, headers=download_headers) as response:
                logger.info(f"ë‹¤ìš´ë¡œë“œ ì‘ë‹µ: {response.status}, Content-Type: {response.headers.get('Content-Type', 'unknown')}")
                
                if response.status != 200:
                    logger.error(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {response.status}")
                    return None
                
                # Content-Type í™•ì¸
                content_type = response.headers.get('Content-Type', '').lower()
                if 'text/html' in content_type:
                    logger.warning(f"HTML ì‘ë‹µ ê°ì§€ë¨: {content_type}")
                    # HTML ë‚´ìš© ì¼ë¶€ ë¡œê·¸
                    try:
                        html_content = await response.text(encoding='utf-8')
                        logger.warning(f"HTML ë‚´ìš© ì¼ë¶€: {html_content[:200]}")
                    except:
                        pass
                    return None
                
                # íŒŒì¼ ì €ì¥
                async with aiofiles.open(file_path, 'wb') as f:
                    async for chunk in response.content.iter_chunked(8192):
                        await f.write(chunk)
                
                # ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì´ ì‹¤ì œ Excel íŒŒì¼ì¸ì§€ í™•ì¸
                if await self._verify_excel_file(file_path):
                    logger.info(f"ğŸ’¾ Excel íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {file_name}")
                    return file_path
                else:
                    # HTML íŒŒì¼ì´ë‚˜ ì˜ëª»ëœ íŒŒì¼ì¸ ê²½ìš° ì‚­ì œ
                    file_path.unlink()
                    logger.warning(f"âŒ ì˜ëª»ëœ íŒŒì¼ í˜•ì‹ (HTML/ì˜¤ë¥˜ í˜ì´ì§€): {file_name}")
                    return None
        
        except Exception as e:
            logger.error(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    async def _verify_excel_file(self, file_path: Path) -> bool:
        """ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì´ ì‹¤ì œ Excel íŒŒì¼ì¸ì§€ í™•ì¸"""
        try:
            # íŒŒì¼ í¬ê¸° í™•ì¸ (ë„ˆë¬´ ì‘ìœ¼ë©´ ì˜¤ë¥˜ í˜ì´ì§€ì¼ ê°€ëŠ¥ì„±)
            if file_path.stat().st_size < 1024:  # 1KB ë¯¸ë§Œ
                return False
            
            # íŒŒì¼ ì‹œê·¸ë‹ˆì²˜ í™•ì¸ (Excel íŒŒì¼ì€ ZIP ê¸°ë°˜)
            with open(file_path, 'rb') as f:
                header = f.read(4)
                # Excel 2007+ íŒŒì¼ì€ ZIP í˜•ì‹ (PKë¡œ ì‹œì‘)
                if header.startswith(b'PK'):
                    return True
                # Excel 97-2003 íŒŒì¼ ì‹œê·¸ë‹ˆì²˜
                elif header.startswith(b'\xd0\xcf\x11\xe0'):
                    return True
                # HTML íŒŒì¼ ì²´í¬
                elif header.startswith(b'<!DO') or header.startswith(b'<htm'):
                    return False
            
            return False
        except Exception:
            return False
    
    async def _get_related_files(self, soup: BeautifulSoup) -> List[str]:
        """ì—°ê´€ íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘"""
        related_files = []
        
        try:
            # ì—°ê´€ íŒŒì¼ ì„¹ì…˜ ì°¾ê¸°
            related_section = soup.find('div', class_='swiper-box reco')
            if not related_section:
                return related_files
            
            # ì—°ê´€ íŒŒì¼ ë§í¬ ìˆ˜ì§‘
            for link in related_section.find_all('a', href=True):
                href = link.get('href')
                if href and 'view.php' in href:
                    related_files.append(urljoin(self.base_url, href))
        
        except Exception as e:
            logger.error(f"ì—°ê´€ íŒŒì¼ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        return related_files
    
    async def _save_crawling_results(self):
        """í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥"""
        logger.info("ğŸ’¾ í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥ ì¤‘...")
        
        try:
            # JSON í˜•íƒœë¡œ ì €ì¥
            results = {
                'crawling_info': {
                    'start_time': self.progress['start_time'].isoformat(),
                    'end_time': datetime.now().isoformat(),
                    'total_templates': len(self.crawled_templates),
                    'successful_downloads': self.progress['downloaded_templates'],
                    'failed_downloads': self.progress['failed_downloads']
                },
                'templates': [asdict(template) for template in self.crawled_templates],
                'failed_downloads': self.failed_downloads
            }
            
            results_file = self.metadata_dir / f"crawling_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            
            async with aiofiles.open(results_file, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(results, ensure_ascii=False, indent=2))
            
            logger.info(f"âœ… í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {results_file}")
        
        except Exception as e:
            logger.error(f"âŒ í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def _generate_summary_report(self):
        """ìš”ì•½ ë³´ê³ ì„œ ìƒì„±"""
        logger.info("ğŸ“Š ìš”ì•½ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        try:
            # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
            category_stats = {}
            for template in self.crawled_templates:
                major = template.category_major
                minor = template.category_minor
                
                if major not in category_stats:
                    category_stats[major] = {}
                if minor not in category_stats[major]:
                    category_stats[major][minor] = 0
                
                category_stats[major][minor] += 1
            
            # ë³´ê³ ì„œ ë‚´ìš© ìƒì„±
            report_content = f"""
# í•œêµ­ì–´ ì—‘ì…€ í…œí”Œë¦¿ í¬ë¡¤ë§ ë³´ê³ ì„œ

## í¬ë¡¤ë§ ê°œìš”
- ì‹œì‘ ì‹œê°„: {self.progress['start_time']}
- ì¢…ë£Œ ì‹œê°„: {datetime.now()}
- ì´ í…œí”Œë¦¿ ìˆ˜: {len(self.crawled_templates)}
- ì„±ê³µí•œ ë‹¤ìš´ë¡œë“œ: {self.progress['downloaded_templates']}
- ì‹¤íŒ¨í•œ ë‹¤ìš´ë¡œë“œ: {self.progress['failed_downloads']}

## ì¹´í…Œê³ ë¦¬ë³„ í†µê³„

"""
            
            for major, minors in category_stats.items():
                report_content += f"### {major}\n"
                total_in_major = sum(minors.values())
                report_content += f"- ì´ {total_in_major}ê°œ í…œí”Œë¦¿\n"
                
                for minor, count in minors.items():
                    report_content += f"  - {minor}: {count}ê°œ\n"
                report_content += "\n"
            
            # ì‹¤íŒ¨í•œ ë‹¤ìš´ë¡œë“œ ëª©ë¡
            if self.failed_downloads:
                report_content += "## ì‹¤íŒ¨í•œ ë‹¤ìš´ë¡œë“œ\n\n"
                for failed in self.failed_downloads:
                    report_content += f"- {failed.get('title', 'Unknown')}\n"
                    report_content += f"  - URL: {failed.get('source_url', 'Unknown')}\n"
            
            # ë³´ê³ ì„œ íŒŒì¼ ì €ì¥
            report_file = self.reports_dir / f"crawling_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
            
            async with aiofiles.open(report_file, 'w', encoding='utf-8') as f:
                await f.write(report_content)
            
            logger.info(f"âœ… ìš”ì•½ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {report_file}")
        
        except Exception as e:
            logger.error(f"âŒ ìš”ì•½ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
    
    def get_progress(self) -> Dict[str, Any]:
        """í˜„ì¬ ì§„í–‰ ìƒí™© ë°˜í™˜"""
        if self.progress['start_time']:
            duration = datetime.now() - self.progress['start_time']
            self.progress['duration'] = str(duration)
        
        return self.progress.copy()


# ì „ì—­ í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤
korean_template_crawler = KoreanTemplateCrawler()