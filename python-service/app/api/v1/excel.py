"""
Excel analysis API endpoints
"""

from fastapi import APIRouter, File, UploadFile, HTTPException, Depends
from pydantic import BaseModel, Field
from sqlalchemy.ext.asyncio import AsyncSession
import tempfile
import os
from typing import Dict, Any, List, Optional
import logging
from datetime import datetime

from app.core.database import get_db
from app.core.config import settings
from app.core.i18n_dependencies import get_i18n_context, I18nContext
from app.core.validators import FileValidator
from app.services.excel_analyzer import excel_analyzer
from app.services.openai_service import openai_service
from app.services.vector_search import vector_search_service
from app.services.excel_chart_analyzer import excel_chart_analyzer
from app.services.excel_pivot_analyzer import excel_pivot_analyzer
from app.services.template_selection_service import template_selection_service
from app.services.excel_auto_fixer import ExcelAutoFixer
from app.services.circular_reference_detector import CircularReferenceDetector
from app.services.fast_formula_fixer import FastFormulaFixer
from app.services.context import get_enhanced_context_manager
from app.core.responses import ResponseBuilder

logger = logging.getLogger(__name__)

router = APIRouter()


class AnalyzeRequest(BaseModel):
    user_query: Optional[str] = Field(None, max_length=1000)
    session_id: Optional[str] = Field(
        None, pattern=r"^[a-zA-Z0-9\-_]+$", max_length=128
    )


@router.post("/analyze")
async def analyze_excel_file(
    file: UploadFile = File(...),
    request: AnalyzeRequest = AnalyzeRequest(),
    db: AsyncSession = Depends(get_db),
    i18n: I18nContext = Depends(get_i18n_context),
) -> Dict[str, Any]:
    """
    Analyze an Excel file and optionally answer a specific query
    """
    # Enhanced file validation
    if not FileValidator.validate_filename(file.filename):
        error_message = i18n.get_error_message(
            "invalid_file_type", extensions=", ".join(settings.ALLOWED_EXTENSIONS)
        )
        raise HTTPException(status_code=400, detail=error_message)

    # Validate file size
    if not FileValidator.validate_file_size(file.size):
        error_message = i18n.get_error_message(
            "file_too_large", max_size=settings.MAX_UPLOAD_SIZE
        )
        raise HTTPException(status_code=400, detail=error_message)

    # Validate MIME type by reading first bytes
    content = await file.read()
    await file.seek(0)  # Reset file pointer

    if not FileValidator.validate_mime_type(file.filename, content[:8]):
        raise HTTPException(
            status_code=400,
            detail="Invalid file format. Please upload a valid Excel file.",
        )

    # Save uploaded file temporarily
    with tempfile.NamedTemporaryFile(
        delete=False, suffix=os.path.splitext(file.filename)[1]
    ) as tmp_file:
        content = await file.read()
        tmp_file.write(content)
        tmp_path = tmp_file.name

    try:
        # Generate unique file ID and save mapping
        from app.core.file_path_resolver import FilePathResolver

        file_id = await FilePathResolver.generate_file_id(file.filename)
        await FilePathResolver.save_file_mapping(
            file_id,
            tmp_path,
            {
                "original_filename": file.filename,
                "size": file.size,
                "session_id": request.session_id,
            },
        )

        # Analyze file structure using IntegratedErrorDetector
        logger.info(f"Analyzing file: {file.filename} (ID: {file_id})")
        from app.services.detection.integrated_error_detector import (
            IntegratedErrorDetector,
        )

        # Detect errors with IntegratedErrorDetector
        detector = IntegratedErrorDetector()
        detection_result = await detector.detect_all_errors(tmp_path)
        errors = detection_result["errors"]

        # Also get basic file analysis from excel_analyzer for compatibility
        analysis_result = await excel_analyzer.analyze_file(tmp_path)

        # Add detected errors to analysis result
        analysis_result["errors"] = [
            {
                "id": error.id,
                "type": error.type,
                "sheet": error.sheet,
                "cell": error.cell,
                "message": error.message,
                "severity": error.severity,
                "auto_fixable": error.is_auto_fixable,  # Changed from is_auto_fixable to auto_fixable for Rails compatibility
                "suggested_fix": error.suggested_fix,
            }
            for error in errors
        ]

        # Update summary with error count
        analysis_result["summary"]["total_errors"] = len(errors)
        analysis_result["summary"]["has_errors"] = len(errors) > 0

        # Get AI insights
        ai_analysis = await openai_service.analyze_excel_content(
            analysis_result, request.user_query
        )

        # Í≥†Í∏â Í∏∞Îä• Î∂ÑÏÑù Ï∂îÍ∞Ä
        advanced_analysis = {}

        # Ï∞®Ìä∏ Ï†úÏïà
        chart_suggestions = excel_chart_analyzer.suggest_optimal_charts(tmp_path)
        advanced_analysis["chart_suggestions"] = chart_suggestions

        # ÌîºÎ≤óÌÖåÏù¥Î∏î Ï†úÏïà
        pivot_suggestions = excel_pivot_analyzer.suggest_optimal_pivots(tmp_path)
        advanced_analysis["pivot_suggestions"] = pivot_suggestions

        # ÌÖúÌîåÎ¶ø Ï∂îÏ≤ú (ÏÇ¨Ïö©Ïûê ÏøºÎ¶¨Í∞Ä ÏûàÎäî Í≤ΩÏö∞)
        if request.user_query:
            template_recommendations = (
                await template_selection_service.recommend_templates(
                    user_intent=request.user_query,
                    excel_file_path=tmp_path,
                    max_recommendations=3,
                )
            )
            advanced_analysis["template_recommendations"] = template_recommendations

        # Initialize workbook context if session_id is provided
        if request.session_id:
            context_manager = get_enhanced_context_manager()
            try:
                # Î∂ÑÏÑù Í≤∞Í≥º Íµ¨Ï°∞ Î≥ÄÌôò (detection_resultÏôÄ analysis_result ÌÜµÌï©)
                combined_result = {
                    **analysis_result,
                    "errors": detection_result["errors"],
                    "summary": {
                        **analysis_result.get("summary", {}),
                        **detection_result.get("summary", {}),
                    },
                }

                workbook_context = await context_manager.initialize_workbook_context(
                    session_id=request.session_id,
                    file_id=file_id,  # Í≥†Ïú† ÌååÏùº ID ÏÇ¨Ïö©
                    file_name=file.filename,
                    analysis_result=combined_result,
                )

                # IntegratedErrorDetector Í≤∞Í≥ºÎ°ú ÏóÖÎç∞Ïù¥Ìä∏
                await context_manager.update_from_detector_result(
                    request.session_id, detection_result
                )

                logger.info(f"ÏõåÌÅ¨Î∂Å Ïª®ÌÖçÏä§Ìä∏ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å: {file.filename}")

                # WebSocketÏúºÎ°ú Î∂ÑÏÑù ÏôÑÎ£å ÏïåÎ¶º
                from app.api.v1.context_websocket import manager as ws_manager

                await ws_manager.send_to_session(
                    request.session_id,
                    {
                        "type": "analysis_complete",
                        "data": {
                            "file_id": file_id,
                            "file_name": file.filename,
                            "total_errors": len(errors),
                            "error_types": list(
                                set(
                                    e["type"]
                                    for e in errors
                                    if isinstance(e, dict) and "type" in e
                                )
                            ),
                            "summary": detection_result.get("summary", {}),
                        },
                        "timestamp": datetime.now().isoformat(),
                    },
                )

            except Exception as e:
                logger.error(f"ÏõåÌÅ¨Î∂Å Ïª®ÌÖçÏä§Ìä∏ Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {str(e)}")
                # Ïã§Ìå®Ìï¥ÎèÑ Í≥ÑÏÜç ÏßÑÌñâ

        # Index content for future searches with enhanced summary
        content_summary = _create_enhanced_content_summary(
            analysis_result, detection_result
        )
        await vector_search_service.index_document(
            document_id=file_id,  # ÌååÏùº ID ÏÇ¨Ïö©
            document_type="excel_analysis",
            content=content_summary,
            metadata={
                "file_id": file_id,
                "filename": file.filename,
                "sheets": list(analysis_result["sheets"].keys()),
                "has_errors": analysis_result["summary"]["has_errors"],
                "error_count": len(errors),
                "error_types": list(
                    set(
                        e["type"] for e in errors if isinstance(e, dict) and "type" in e
                    )
                ),
                "session_id": request.session_id,
            },
            db=db,
        )

        # ÌëúÏ§Ä ÏùëÎãµ ÏÉùÏÑ±
        response_data = {
            "file_id": file_id,
            "filename": file.filename,
            "file_analysis": analysis_result,
            "ai_insights": ai_analysis,
            "advanced_analysis": advanced_analysis,
            "user_query": request.user_query,
            "session_id": request.session_id,
            "language": i18n.language,
            "localized_labels": {
                "charts": i18n.get_text("excel.charts.title"),
                "pivot_tables": i18n.get_text("excel.pivot.title"),
                "templates": i18n.get_text("templates.title"),
            },
        }

        return ResponseBuilder.success(
            data=response_data, message=i18n.get_text("file.analysis_complete")
        )

    except Exception as e:
        logger.error(f"Error analyzing Excel file: {str(e)}")
        error_response = ResponseBuilder.from_exception(
            exception=e,
            context={"filename": file.filename},
            include_traceback=settings.DEBUG,
        )
        raise HTTPException(status_code=500, detail=error_response)
    finally:
        # Clean up temporary file
        if os.path.exists(tmp_path):
            os.unlink(tmp_path)


@router.post("/extract-formulas")
async def extract_formulas(file: UploadFile = File(...)) -> Dict[str, Any]:
    """
    Extract all formulas from an Excel file
    """
    # Validate file type
    if not any(file.filename.endswith(ext) for ext in settings.ALLOWED_EXTENSIONS):
        raise HTTPException(
            status_code=400,
            detail=f"Invalid file type. Allowed types: {settings.ALLOWED_EXTENSIONS}",
        )

    # Save uploaded file temporarily
    with tempfile.NamedTemporaryFile(
        delete=False, suffix=os.path.splitext(file.filename)[1]
    ) as tmp_file:
        content = await file.read()
        tmp_file.write(content)
        tmp_path = tmp_file.name

    try:
        # Extract formulas
        formulas = await excel_analyzer.extract_formulas(tmp_path)

        # Count total formulas
        total_formulas = sum(
            len(sheet_formulas) for sheet_formulas in formulas.values()
        )

        return ResponseBuilder.success(
            data={
                "filename": file.filename,
                "total_formulas": total_formulas,
                "formulas_by_sheet": formulas,
            }
        )

    except Exception as e:
        logger.error(f"Error extracting formulas: {str(e)}")
        error_response = ResponseBuilder.from_exception(
            exception=e, context={"filename": file.filename}
        )
        raise HTTPException(status_code=500, detail=error_response)
    finally:
        # Clean up temporary file
        if os.path.exists(tmp_path):
            os.unlink(tmp_path)


@router.post("/validate-formula")
async def validate_formula(
    formula: str, context: Dict[str, Any] = None
) -> Dict[str, Any]:
    """
    Validate and explain an Excel formula
    """
    if not formula.startswith("="):
        formula = "=" + formula

    # Get AI explanation
    prompt = f"Explain this Excel formula and check for any issues: {formula}"
    if context:
        prompt += f"\nContext: {context}"

    solution = await openai_service.generate_excel_solution(prompt, context)

    return ResponseBuilder.success(
        data={"formula": formula, "validation": solution, "context": context}
    )


@router.post("/auto-fix")
async def auto_fix_excel_file(
    file: UploadFile = File(...),
    fix_formulas: bool = True,
    fix_data_quality: bool = True,
    fix_structural: bool = True,
    fix_formatting: bool = True,
    save_fixed_file: bool = True,
    db: AsyncSession = Depends(get_db),
    i18n: I18nContext = Depends(get_i18n_context),
) -> Dict[str, Any]:
    """
    ÏûêÎèôÏúºÎ°ú Excel ÌååÏùºÏùò Î™®Îì† Ïò§Î•òÎ•º Í∞êÏßÄÌïòÍ≥† ÏàòÏ†ï
    """
    # ÌååÏùº ÌÉÄÏûÖ Í≤ÄÏ¶ù
    if not any(file.filename.endswith(ext) for ext in settings.ALLOWED_EXTENSIONS):
        error_message = i18n.get_error_message(
            "invalid_file_type", extensions=", ".join(settings.ALLOWED_EXTENSIONS)
        )
        raise HTTPException(status_code=400, detail=error_message)

    # ÌååÏùº ÌÅ¨Í∏∞ Í≤ÄÏ¶ù
    if file.size > settings.MAX_UPLOAD_SIZE:
        error_message = i18n.get_error_message(
            "file_too_large", max_size=settings.MAX_UPLOAD_SIZE
        )
        raise HTTPException(status_code=400, detail=error_message)

    # ÏûÑÏãú ÌååÏùº Ï†ÄÏû•
    with tempfile.NamedTemporaryFile(
        delete=False, suffix=os.path.splitext(file.filename)[1]
    ) as tmp_file:
        content = await file.read()
        tmp_file.write(content)
        tmp_path = tmp_file.name

    try:
        # ÏûêÎèô ÏàòÏ†ï ÏòµÏÖò ÏÑ§Ï†ï
        fix_options = {
            "fix_formulas": fix_formulas,
            "fix_data_quality": fix_data_quality,
            "fix_structural": fix_structural,
            "fix_formatting": fix_formatting,
            "save_fixed_file": save_fixed_file,
            "revalidate": True,
        }

        # ÏûêÎèô ÏàòÏ†ï Ïã§Ìñâ
        logger.info(f"Starting auto-fix for file: {file.filename}")
        auto_fixer = ExcelAutoFixer()
        fix_result = await auto_fixer.auto_fix_file(tmp_path, fix_options)

        # Í≤∞Í≥º ÏöîÏïΩ
        response_data = {
            "filename": file.filename,
            "summary": fix_result["summary"],
            "performance": fix_result["performance"],
            "details": {
                "original_errors": fix_result["original_errors"].get("summary", {}),
                "fixed_errors": {
                    "formulas": fix_result["fixed_errors"]
                    .get("formulas", {})
                    .get("fixed_formulas", []),
                    "data_quality": fix_result["fixed_errors"]
                    .get("data_quality", {})
                    .get("fixes_applied", []),
                    "structural": fix_result["fixed_errors"]
                    .get("structural", {})
                    .get("fixes_applied", []),
                    "formatting": fix_result["fixed_errors"]
                    .get("formatting", {})
                    .get("fixes_applied", []),
                },
                "remaining_errors": len(fix_result.get("unfixed_errors", [])),
            },
            "language": i18n.language,
        }

        # ÏàòÏ†ïÎêú ÌååÏùº Í≤ΩÎ°ú Ï∂îÍ∞Ä
        if "fixed_file_path" in fix_result:
            response_data["fixed_file_path"] = fix_result["fixed_file_path"]

        # Î≤°ÌÑ∞ DBÏóê Ïù∏Îç±Ïã±
        await vector_search_service.index_document(
            document_id=f"{file.filename}_autofix",
            document_type="excel_autofix",
            content=f"Auto-fixed Excel file: {file.filename}. Fixed {fix_result['summary']['total_errors_fixed']} errors.",
            metadata={
                "filename": file.filename,
                "total_errors_fixed": fix_result["summary"]["total_errors_fixed"],
                "fix_rate": fix_result["summary"]["fix_rate"],
            },
            db=db,
        )

        return ResponseBuilder.success(
            data=response_data, message=i18n.get_text("file.auto_fix_complete")
        )

    except Exception as e:
        logger.error(f"Error in auto-fix: {str(e)}")
        error_response = ResponseBuilder.from_exception(
            exception=e, context={"filename": file.filename}
        )
        raise HTTPException(status_code=500, detail=error_response)
    finally:
        # ÏûÑÏãú ÌååÏùº Ï†ïÎ¶¨
        if os.path.exists(tmp_path):
            os.unlink(tmp_path)


@router.post("/detect-errors")
async def detect_errors(
    file: UploadFile = File(...), session_id: str = None
) -> Dict[str, Any]:
    """
    Excel ÌååÏùºÏùò Ïò§Î•ò Í∞êÏßÄ - IntegratedErrorDetector ÏÇ¨Ïö©
    """
    # Validate file type
    if not any(file.filename.endswith(ext) for ext in settings.ALLOWED_EXTENSIONS):
        raise HTTPException(
            status_code=400,
            detail=f"Invalid file type. Allowed types: {settings.ALLOWED_EXTENSIONS}",
        )

    # Save uploaded file temporarily
    with tempfile.NamedTemporaryFile(
        delete=False, suffix=os.path.splitext(file.filename)[1]
    ) as tmp_file:
        content = await file.read()
        tmp_file.write(content)
        tmp_path = tmp_file.name

    try:
        # Use IntegratedErrorDetector for comprehensive error detection
        from app.services.detection.integrated_error_detector import (
            IntegratedErrorDetector,
        )

        logger.info(f"Detecting errors in file: {file.filename}")
        detector = IntegratedErrorDetector()
        detection_result = await detector.detect_all_errors(tmp_path)
        errors = detection_result["errors"]

        # Convert errors to dict format
        errors_dict = [
            {
                "id": error.id,
                "type": error.type,
                "sheet": error.sheet,
                "cell": error.cell,
                "message": error.message,
                "severity": error.severity,
                "auto_fixable": error.is_auto_fixable,  # Changed to auto_fixable for Rails compatibility
                "suggested_fix": error.suggested_fix,
                "formula": error.formula,
                "value": error.value,
                "confidence": error.confidence,
            }
            for error in errors
        ]

        # Generate summary
        summary = {
            "total_errors": len(errors),
            "critical_errors": sum(1 for e in errors if e.severity == "critical"),
            "high_errors": sum(1 for e in errors if e.severity == "high"),
            "medium_errors": sum(1 for e in errors if e.severity == "medium"),
            "low_errors": sum(1 for e in errors if e.severity == "low"),
            "auto_fixable": sum(1 for e in errors if e.is_auto_fixable),
            "has_errors": len(errors) > 0,
        }

        return ResponseBuilder.success(
            data={
                "filename": file.filename,
                "errors": errors_dict,
                "summary": summary,
                "session_id": session_id,
            },
            message="Ïò§Î•ò Í∞êÏßÄ ÏôÑÎ£å",
        )

    except Exception as e:
        logger.error(f"Error detecting Excel errors: {str(e)}")
        error_response = ResponseBuilder.from_exception(
            exception=e, context={"filename": file.filename}
        )
        raise HTTPException(status_code=500, detail=error_response)
    finally:
        # Clean up temporary file
        if os.path.exists(tmp_path):
            os.unlink(tmp_path)


def _create_enhanced_content_summary(
    analysis: Dict[str, Any], detection_result: Dict[str, Any]
) -> str:
    """IntegratedErrorDetector Í≤∞Í≥ºÎ•º Ìè¨Ìï®Ìïú Ìñ•ÏÉÅÎêú Ïª®ÌÖêÏ∏† ÏöîÏïΩ ÏÉùÏÑ±"""
    parts = []
    summary = analysis.get("summary", {})

    # ÌååÏùº Í∞úÏöî
    parts.append(
        f"Excel ÌååÏùº Í∞úÏöî: {summary.get('total_sheets', 0)}Í∞ú ÏãúÌä∏, "
        f"{summary.get('total_rows', 0)}Ìñâ, {summary.get('total_cells_with_data', 0)}Í∞ú Îç∞Ïù¥ÌÑ∞ ÏÖÄ"
    )

    # IntegratedErrorDetector Ïò§Î•ò ÏöîÏïΩ
    if detection_result and "summary" in detection_result:
        det_summary = detection_result["summary"]
        parts.append("\nÏò§Î•ò Í∞êÏßÄ Í≤∞Í≥º:")
        parts.append(f"- Ï¥ù Ïò§Î•ò: {det_summary.get('total_errors', 0)}Í∞ú")
        parts.append(
            f"- ÏûêÎèô ÏàòÏ†ï Í∞ÄÎä•: {det_summary.get('auto_fixable', 0)}Í∞ú "
            f"({det_summary.get('auto_fixable_percentage', 0)}%)"
        )

        # Ïò§Î•ò ÌÉÄÏûÖÎ≥Ñ Î∂ÑÎ•ò
        if "by_type" in det_summary:
            parts.append("- Ïò§Î•ò ÌÉÄÏûÖ:")
            for error_type, count in det_summary["by_type"].items():
                parts.append(f"  ¬∑ {error_type}: {count}Í∞ú")

        # Ïã¨Í∞ÅÎèÑÎ≥Ñ Î∂ÑÎ•ò
        if "by_severity" in det_summary:
            parts.append("- Ïã¨Í∞ÅÎèÑ:")
            for severity, count in det_summary["by_severity"].items():
                parts.append(f"  ¬∑ {severity}: {count}Í∞ú")

    # Ìå®ÌÑ¥ Î∂ÑÏÑù Í≤∞Í≥º (ÏûàÎäî Í≤ΩÏö∞)
    if "pattern_analysis" in detection_result:
        patterns = detection_result["pattern_analysis"]
        if patterns.get("patterns"):
            parts.append("\nÎ∞úÍ≤¨Îêú Ìå®ÌÑ¥:")
            for pattern in patterns["patterns"][:3]:  # ÏÉÅÏúÑ 3Í∞ú Ìå®ÌÑ¥
                parts.append(f"- {pattern.get('description', 'Unknown pattern')}")

    # ÏãúÌä∏Î≥Ñ ÏÉÅÏÑ∏ Ï†ïÎ≥¥
    parts.append("\nÏãúÌä∏Î≥Ñ ÏÉÅÏÑ∏:")
    for sheet_name, sheet_data in analysis.get("sheets", {}).items():
        sheet_summary = sheet_data.get("summary", {})
        parts.append(f"\n[{sheet_name}]")
        parts.append(
            f"- ÌÅ¨Í∏∞: {sheet_summary.get('rows', 0)}Ìñâ √ó {sheet_summary.get('columns', 0)}Ïó¥"
        )
        parts.append(f"- Îç∞Ïù¥ÌÑ∞ ÏÖÄ: {sheet_summary.get('non_empty_cells', 0)}Í∞ú")

        # ÏãúÌä∏Î≥Ñ Ïò§Î•ò
        if detection_result and "by_sheet" in detection_result.get("summary", {}):
            sheet_errors = detection_result["summary"]["by_sheet"].get(sheet_name, 0)
            if sheet_errors > 0:
                parts.append(f"- Ïò§Î•ò: {sheet_errors}Í∞ú")

        # Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ Î∂ÑÌè¨
        if "data_types" in sheet_data:
            type_summary = []
            for dtype, count in sheet_data["data_types"].items():
                if count > 0:
                    type_summary.append(f"{dtype}: {count}")
            if type_summary:
                parts.append(f"- Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ: {', '.join(type_summary)}")

        # ÏàòÏãù Ï†ïÎ≥¥
        if sheet_data.get("formulas"):
            parts.append(f"- ÏàòÏãù: {len(sheet_data['formulas'])}Í∞ú")
            # Ï£ºÏöî ÏàòÏãù ÏòàÏãú
            for formula in sheet_data["formulas"][:3]:
                parts.append(f"  ¬∑ {formula['cell']}: {formula['formula'][:30]}...")

    # AIÍ∞Ä Ïù¥Ìï¥ÌïòÍ∏∞ Ïâ¨Ïö¥ Îß•ÎùΩ Ï†ïÎ≥¥ Ï∂îÍ∞Ä
    parts.append("\nÎß•ÎùΩ Ï†ïÎ≥¥:")
    parts.append(
        f"- Ïù¥ ÌååÏùºÏùÄ {summary.get('total_sheets', 0)}Í∞úÏùò ÏõåÌÅ¨ÏãúÌä∏Î°ú Íµ¨ÏÑ±ÎêòÏñ¥ ÏûàÏäµÎãàÎã§."
    )
    if (
        detection_result
        and detection_result.get("summary", {}).get("total_errors", 0) > 0
    ):
        parts.append(
            f"- {detection_result['summary']['total_errors']}Í∞úÏùò Ïò§Î•òÍ∞Ä Î∞úÍ≤¨ÎêòÏóàÏúºÎ©∞, "
            f"Í∞ÄÏû• ÎßéÏùÄ Ïò§Î•ò ÌÉÄÏûÖÏùÄ '{detection_result['summary'].get('most_common_type', 'unknown')}'ÏûÖÎãàÎã§."
        )
    if summary.get("has_charts"):
        parts.append("- Ï∞®Ìä∏Í∞Ä Ìè¨Ìï®ÎêòÏñ¥ ÏûàÏäµÎãàÎã§.")
    if summary.get("has_pivot_tables"):
        parts.append("- ÌîºÎ≤ó ÌÖåÏù¥Î∏îÏù¥ Ìè¨Ìï®ÎêòÏñ¥ ÏûàÏäµÎãàÎã§.")

    return "\n".join(parts)


def _create_content_summary(analysis: Dict[str, Any]) -> str:
    """Create a comprehensive searchable summary of Excel file content with detailed context"""
    parts = []
    summary = analysis.get("summary", {})

    # File overview
    parts.append(
        f"Excel ÌååÏùº Í∞úÏöî: {summary.get('total_sheets', 0)}Í∞ú ÏãúÌä∏, "
        f"{summary.get('total_rows', 0)}Ìñâ, {summary.get('total_cells_with_data', 0)}Í∞ú Îç∞Ïù¥ÌÑ∞ ÏÖÄ"
    )

    # Error summary if present
    if summary.get("has_errors"):
        parts.append(f"\nÏ¥ù {summary.get('total_errors', 0)}Í∞ú Ïò§Î•ò Î∞úÍ≤¨:")
        if "errors" in analysis:
            error_types = {}
            for error in analysis["errors"][:10]:  # First 10 errors for context
                error_types[error["type"]] = error_types.get(error["type"], 0) + 1
            for error_type, count in error_types.items():
                parts.append(f"- {error_type}: {count}Í∞ú")

    # Sheet details
    parts.append("\nÏãúÌä∏Î≥Ñ ÏÉÅÏÑ∏:")
    for sheet_name, sheet_data in analysis.get("sheets", {}).items():
        parts.append(f"\n[{sheet_name}]")

        # Column information
        if "columns" in sheet_data:
            cols = sheet_data["columns"][:15]  # More columns for better context
            parts.append(f"Ïó¥: {', '.join(cols)}")
            if len(sheet_data["columns"]) > 15:
                parts.append(f"... Ïô∏ {len(sheet_data['columns']) - 15}Í∞ú Ïó¥")

        # Data type analysis
        if "column_analysis" in sheet_data:
            data_types = {}
            for col_info in sheet_data["column_analysis"]:
                dtype = col_info.get("data_type", "unknown")
                data_types[dtype] = data_types.get(dtype, 0) + 1
            if data_types:
                parts.append(
                    "Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ: "
                    + ", ".join([f"{k}({v})" for k, v in data_types.items()])
                )

        # Formulas
        if sheet_data.get("formula_count", 0) > 0:
            parts.append(f"ÏàòÏãù {sheet_data['formula_count']}Í∞ú Ìè¨Ìï®")
            if "complex_formulas" in sheet_data:
                parts.append(
                    f"Î≥µÏû°Ìïú ÏàòÏãù: {', '.join(sheet_data['complex_formulas'][:3])}"
                )

        # Data quality
        if "data_quality_issues" in sheet_data:
            parts.append(
                f"Îç∞Ïù¥ÌÑ∞ ÌíàÏßà Ïù¥Ïäà: {len(sheet_data['data_quality_issues'])}Í∞ú"
            )

        # Key metrics if identified
        if "key_metrics" in sheet_data:
            parts.append(f"Ï£ºÏöî ÏßÄÌëú: {', '.join(sheet_data['key_metrics'][:5])}")

    # Business context hints (inferred from column names and data)
    business_keywords = _infer_business_context(analysis)
    if business_keywords:
        parts.append(f"\nÎπÑÏ¶àÎãàÏä§ Ïª®ÌÖçÏä§Ìä∏: {', '.join(business_keywords)}")

    return " ".join(parts)


def _infer_business_context(analysis: Dict[str, Any]) -> List[str]:
    """Infer business context from column names and data patterns"""
    keywords = set()
    business_terms = {
        "Îß§Ï∂ú": ["sales", "revenue", "Îß§Ï∂ú", "ÏàòÏùµ"],
        "ÎπÑÏö©": ["cost", "expense", "ÎπÑÏö©", "ÏßÄÏ∂ú"],
        "Ïù¥Ïùµ": ["profit", "margin", "Ïù¥Ïùµ", "ÎßàÏßÑ"],
        "Ïû¨Í≥†": ["inventory", "stock", "Ïû¨Í≥†", "ÏûÖÍ≥†", "Ï∂úÍ≥†"],
        "Í≥†Í∞ù": ["customer", "client", "Í≥†Í∞ù", "Í±∞ÎûòÏ≤ò"],
        "ÎÇ†Ïßú": ["date", "time", "ÎÇ†Ïßú", "ÏùºÏûê", "ÎÖÑ", "Ïõî"],
    }

    # Check all column names across sheets
    for sheet_data in analysis.get("sheets", {}).values():
        for col in sheet_data.get("columns", []):
            col_lower = col.lower()
            for category, terms in business_terms.items():
                if any(term in col_lower for term in terms):
                    keywords.add(category)

    return list(keywords)


@router.post("/detect-circular-references")
async def detect_circular_references(
    file_path: str, i18n: I18nContext = Depends(get_i18n_context)
) -> Dict[str, Any]:
    """
    Detect circular references in Excel file using advanced graph analysis
    """
    try:
        import openpyxl

        # Load workbook
        workbook = openpyxl.load_workbook(file_path, data_only=False)

        # Initialize detector
        detector = CircularReferenceDetector()

        # Analyze workbook
        circular_chains = detector.analyze_workbook(workbook)

        # Convert to JSON-serializable format
        results = []
        for chain in circular_chains:
            results.append(
                {
                    "cells": chain.cells,
                    "chain_type": chain.chain_type,
                    "severity": chain.severity,
                    "description": chain.description,
                    "break_suggestions": chain.break_suggestions,
                }
            )

        return {
            "circular_references": results,
            "total_found": len(results),
            "message": i18n.get_message(
                "circular_references_detected", count=len(results)
            ),
        }

    except Exception as e:
        logger.error(f"Circular reference detection failed: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=i18n.get_error_message("analysis_failed", error=str(e)),
        )


@router.post("/fix-formula")
async def fix_formula(
    formula: str, error_type: str, i18n: I18nContext = Depends(get_i18n_context)
) -> Dict[str, Any]:
    """
    Fix a formula error using FastFormulaFixer
    """
    try:
        # Initialize fixer
        fixer = FastFormulaFixer()

        # Map error description to error type
        error_mapping = {
            "#DIV/0!": "#DIV/0!",
            "#N/A": "#N/A",
            "#NAME?": "#NAME?",
            "#REF!": "#REF!",
            "#VALUE!": "#VALUE!",
            "#NUM!": "#NUM!",
            "#NULL!": "#NULL!",
            "#SPILL!": "#SPILL!",
            "#CALC!": "#CALC!",
        }

        # Find matching error type
        detected_error = None
        for error, code in error_mapping.items():
            if error in error_type:
                detected_error = code
                break

        if not detected_error:
            detected_error = "#VALUE!"  # Default

        # Fix formula
        result = fixer.fix_formula(formula, detected_error)

        return {
            "original_formula": formula,
            "fixed_formula": result.fixed_formula,
            "confidence": result.confidence,
            "explanation": result.explanation,
            "error_type": detected_error,
        }

    except Exception as e:
        logger.error(f"Formula fix failed: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=i18n.get_error_message("formula_fix_failed", error=str(e)),
        )


@router.post("/update-cell")
async def update_cell(
    file_path: str,
    location: str,
    value: str,
    i18n: I18nContext = Depends(get_i18n_context),
) -> Dict[str, Any]:
    """
    Update a specific cell in Excel file
    """
    try:
        import openpyxl

        # Load workbook
        workbook = openpyxl.load_workbook(file_path)

        # Parse location (e.g., "Sheet1!A1" or "A1")
        if "!" in location:
            sheet_name, cell_ref = location.split("!", 1)
            sheet = workbook[sheet_name]
        else:
            sheet = workbook.active
            cell_ref = location

        # Update cell
        sheet[cell_ref] = value

        # Save workbook
        workbook.save(file_path)
        workbook.close()

        return {
            "success": True,
            "location": location,
            "new_value": value,
            "message": i18n.get_message("cell_updated", location=location),
        }

    except Exception as e:
        logger.error(f"Cell update failed: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=i18n.get_error_message("cell_update_failed", error=str(e)),
        )


@router.post("/remove-duplicates")
async def remove_duplicates(
    file_path: str, i18n: I18nContext = Depends(get_i18n_context)
) -> Dict[str, Any]:
    """
    Remove duplicate rows from Excel file
    """
    try:
        import pandas as pd

        # Read Excel file
        df = pd.read_excel(file_path)

        # Count duplicates before removal
        duplicates_before = df.duplicated().sum()

        # Remove duplicates
        df_cleaned = df.drop_duplicates()

        # Save back to Excel
        df_cleaned.to_excel(file_path, index=False)

        return {
            "success": True,
            "duplicates_removed": duplicates_before,
            "rows_before": len(df),
            "rows_after": len(df_cleaned),
            "message": i18n.get_message("duplicates_removed", count=duplicates_before),
        }

    except Exception as e:
        logger.error(f"Duplicate removal failed: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=i18n.get_error_message("duplicate_removal_failed", error=str(e)),
        )


@router.post("/optimize-formulas")
async def optimize_formulas(
    file_path: str, i18n: I18nContext = Depends(get_i18n_context)
) -> Dict[str, Any]:
    """
    Optimize formulas in Excel file for better performance
    """
    try:
        import openpyxl

        # Load workbook
        workbook = openpyxl.load_workbook(file_path, data_only=False)

        # Initialize fixer for optimization
        fixer = FastFormulaFixer()

        optimized_count = 0
        optimization_details = []

        for sheet_name in workbook.sheetnames:
            sheet = workbook[sheet_name]

            for row in sheet.iter_rows():
                for cell in row:
                    if cell.data_type == "f" and cell.value:
                        original_formula = cell.value

                        # Check if formula can be optimized
                        if fixer._is_inefficient_formula(original_formula):
                            optimized = fixer._optimize_formula(original_formula)

                            if optimized != original_formula:
                                cell.value = optimized
                                optimized_count += 1

                                optimization_details.append(
                                    {
                                        "location": f"{sheet_name}!{cell.coordinate}",
                                        "original": original_formula,
                                        "optimized": optimized,
                                    }
                                )

        # Save workbook
        workbook.save(file_path)
        workbook.close()

        return {
            "success": True,
            "formulas_optimized": optimized_count,
            "details": optimization_details[:10],  # First 10 optimizations
            "message": i18n.get_message("formulas_optimized", count=optimized_count),
        }

    except Exception as e:
        logger.error(f"Formula optimization failed: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=i18n.get_error_message("optimization_failed", error=str(e)),
        )


class CellVerificationRequest(BaseModel):
    file_id: str
    cell_address: str


@router.post("/validate-and-report")
async def validate_and_report(
    file: UploadFile = File(...),
    session_id: str = None,
    user_id: str = None,
    db: AsyncSession = Depends(get_db),
    i18n: I18nContext = Depends(get_i18n_context),
) -> Dict[str, Any]:
    """
    ÏÇ¨Ïö©ÏûêÍ∞Ä Í≤ÄÏ¶ù Î≤ÑÌäº ÌÅ¥Î¶≠ Ïãú Excel ÌååÏùºÏùÑ Ï†ÑÏ≤¥ Í≤ÄÏ¶ùÌïòÍ≥† AI Ï±ÑÌåÖÏóê Î≥¥Í≥†
    IntegratedErrorDetectorÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Ìè¨Í¥ÑÏ†ÅÏù∏ Ïò§Î•ò Í∞êÏßÄ ÏàòÌñâ
    """
    # ÌååÏùº ÌÉÄÏûÖ Í≤ÄÏ¶ù
    if not any(file.filename.endswith(ext) for ext in settings.ALLOWED_EXTENSIONS):
        error_message = i18n.get_error_message(
            "invalid_file_type", extensions=", ".join(settings.ALLOWED_EXTENSIONS)
        )
        raise HTTPException(status_code=400, detail=error_message)

    # ÌååÏùº ÌÅ¨Í∏∞ Í≤ÄÏ¶ù
    if file.size > settings.MAX_UPLOAD_SIZE:
        error_message = i18n.get_error_message(
            "file_too_large", max_size=settings.MAX_UPLOAD_SIZE
        )
        raise HTTPException(status_code=400, detail=error_message)

    # ÏûÑÏãú ÌååÏùº Ï†ÄÏû•
    with tempfile.NamedTemporaryFile(
        delete=False, suffix=os.path.splitext(file.filename)[1]
    ) as tmp_file:
        content = await file.read()
        tmp_file.write(content)
        tmp_path = tmp_file.name

    try:
        # IntegratedErrorDetector ÏÇ¨Ïö©ÌïòÏó¨ Ï†ÑÏ≤¥ Í≤ÄÏ¶ù
        from app.services.detection.integrated_error_detector import (
            IntegratedErrorDetector,
        )
        from app.services.error_classification_service import ErrorClassificationService
        from app.services.fix_recommendation_service import FixRecommendationService

        logger.info(f"Starting validation for file: {file.filename}")

        # 1. Ïò§Î•ò Í∞êÏßÄ
        detector = IntegratedErrorDetector()
        detection_result = await detector.detect_all_errors(tmp_path)
        errors = detection_result["errors"]

        # 2. Ïò§Î•ò Î∂ÑÎ•ò Î∞è Ïö∞ÏÑ†ÏàúÏúÑ ÏßÄÏ†ï
        classifier = ErrorClassificationService()
        classified_errors = classifier.classify_errors(errors)
        prioritized_errors = classifier.prioritize_errors(errors)
        error_summary = classifier.get_error_summary(errors)

        # 3. ÏàòÏ†ï Ï†úÏïà ÏÉùÏÑ±
        recommender = FixRecommendationService()
        recommendations = recommender.batch_recommendations(prioritized_errors)

        # 4. AI Î≥¥Í≥†ÏÑú ÏÉùÏÑ±
        validation_report = await _generate_validation_report(
            filename=file.filename,
            errors=prioritized_errors,
            error_summary=error_summary,
            classified_errors=classified_errors,
            recommendations=recommendations,
            i18n=i18n,
        )

        # 5. Í≤∞Í≥º Î∞òÌôò (AI Ï±ÑÌåÖÏóê Ï†ÑÎã¨Îê† Îç∞Ïù¥ÌÑ∞)
        response = {
            "status": "success",
            "message": i18n.get_text("validation.complete"),
            "filename": file.filename,
            "validation_results": {
                "total_errors": len(errors),
                "error_summary": error_summary,
                "classified_errors": {
                    category: len(errors_list)
                    for category, errors_list in classified_errors.items()
                },
                "critical_errors": [
                    {
                        "id": error.id,
                        "type": error.type,
                        "location": f"{error.sheet}!{error.cell}",
                        "message": error.message,
                    }
                    for error in prioritized_errors[:5]  # ÏÉÅÏúÑ 5Í∞ú Ï§ëÏöî Ïò§Î•ò
                    if error.severity == "critical"
                ],
                "auto_fixable_count": error_summary.get("auto_fixable", 0),
            },
            "ai_report": validation_report,
            "session_id": session_id,
            "user_id": user_id,
            "timestamp": datetime.utcnow().isoformat(),
        }

        # 6. Î≤°ÌÑ∞ DBÏóê Í≤ÄÏ¶ù Í≤∞Í≥º Ï†ÄÏû•
        await vector_search_service.index_document(
            document_id=f"{file.filename}_validation_{session_id}",
            document_type="excel_validation",
            content=validation_report,
            metadata={
                "filename": file.filename,
                "total_errors": len(errors),
                "critical_errors": error_summary.get("critical_count", 0),
                "session_id": session_id,
                "user_id": user_id,
            },
            db=db,
        )

        # 7. AI Ï±ÑÌåÖÏóê Î≥¥Í≥†ÏÑú Ï†ÑÏÜ°
        if session_id and response.get("ai_report"):
            await _report_to_ai_chat(
                session_id=session_id,
                user_id=user_id,
                report=response["ai_report"],
                error_count=len(errors),
                filename=file.filename,
                db=db,
            )

        logger.info(
            f"Validation complete for {file.filename}: {len(errors)} errors found and reported to AI chat"
        )

        return response

    except Exception as e:
        logger.error(f"Error in validate-and-report: {str(e)}")
        error_message = i18n.get_error_message("validation_failed", error=str(e))
        raise HTTPException(status_code=500, detail=error_message)
    finally:
        # ÏûÑÏãú ÌååÏùº Ï†ïÎ¶¨
        if os.path.exists(tmp_path):
            os.unlink(tmp_path)


async def _generate_validation_report(
    filename: str,
    errors: List[Any],
    error_summary: Dict[str, Any],
    classified_errors: Dict[str, List[Any]],
    recommendations: Dict[str, Any],
    i18n: I18nContext,
) -> str:
    """
    AIÍ∞Ä Ï±ÑÌåÖÏ∞ΩÏóê Î≥¥Í≥†Ìï† Í≤ÄÏ¶ù Í≤∞Í≥º ÏÉùÏÑ±
    """
    # OpenRouterÎ•º ÏÇ¨Ïö©ÌïòÏó¨ Î≥¥Í≥†ÏÑú ÏÉùÏÑ±
    from app.services.openai_service import openai_service

    # Î≥¥Í≥†ÏÑú Ïª®ÌÖçÏä§Ìä∏ Ï§ÄÎπÑ
    context = {
        "filename": filename,
        "total_errors": len(errors),
        "error_summary": error_summary,
        "categories": list(classified_errors.keys()),
        "critical_errors": sum(1 for e in errors if e.severity == "critical"),
        "auto_fixable": error_summary.get("auto_fixable", 0),
        "language": i18n.language,
    }

    # Ï£ºÏöî Ïò§Î•ò ÏÑ§Î™Ö
    critical_errors_desc = []
    for error in errors[:10]:  # ÏÉÅÏúÑ 10Í∞ú
        if error.severity in ["critical", "high"]:
            critical_errors_desc.append(
                f"- {error.type} at {error.sheet}!{error.cell}: {error.message}"
            )

    # AIÏóêÍ≤å Î≥¥Í≥†ÏÑú ÏÉùÏÑ± ÏöîÏ≤≠
    prompt = f"""
    Excel ÌååÏùº '{filename}'Ïùò Í≤ÄÏ¶ùÏù¥ ÏôÑÎ£åÎêòÏóàÏäµÎãàÎã§.
    ÏÇ¨Ïö©ÏûêÏóêÍ≤å ÏπúÍ∑ºÌïòÍ≥† Ïù¥Ìï¥ÌïòÍ∏∞ Ïâ¨Ïö¥ Î≥¥Í≥†ÏÑúÎ•º ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.

    Í≤ÄÏ¶ù Í≤∞Í≥º:
    - Ï¥ù {context['total_errors']}Í∞úÏùò Ïò§Î•ò Î∞úÍ≤¨
    - Ï§ëÏöî Ïò§Î•ò: {context['critical_errors']}Í∞ú
    - ÏûêÎèô ÏàòÏ†ï Í∞ÄÎä•: {context['auto_fixable']}Í∞ú

    Ïò§Î•ò Ïπ¥ÌÖåÍ≥†Î¶¨: {', '.join(context['categories'])}

    Ï£ºÏöî Ïò§Î•ò:
    {chr(10).join(critical_errors_desc[:5])}

    Î≥¥Í≥†ÏÑúÎäî Îã§Ïùå Íµ¨Ï°∞Î°ú ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî:
    1. Ï†ÑÏ≤¥ ÏöîÏïΩ
    2. Ï£ºÏöî Î∞úÍ≤¨ÏÇ¨Ìï≠
    3. Í∂åÏû• Ï°∞ÏπòÏÇ¨Ìï≠
    4. Îã§Ïùå Îã®Í≥Ñ

    ÏÇ¨Ïö©ÏûêÍ∞Ä ÏâΩÍ≤å Ïù¥Ìï¥Ìï† Ïàò ÏûàÎèÑÎ°ù Í∏∞Ïà†Ï†ÅÏù∏ Ïö©Ïñ¥Îäî ÌîºÌïòÍ≥†,
    Íµ¨Ï≤¥Ï†ÅÏù∏ Í∞úÏÑ† Î∞©Î≤ïÏùÑ Ï†úÏãúÌï¥Ï£ºÏÑ∏Ïöî.
    Ïñ∏Ïñ¥: {i18n.language}
    """

    try:
        report = await openai_service.generate_excel_solution(prompt, context)
        return report
    except Exception as e:
        # OpenRouter Ïã§Ìå® Ïãú Í∏∞Î≥∏ Î≥¥Í≥†ÏÑú ÏÉùÏÑ±
        logger.warning(f"AI report generation failed: {e}")
        return _generate_fallback_report(context, critical_errors_desc, i18n)


async def _report_to_ai_chat(
    session_id: str,
    user_id: str,
    report: str,
    error_count: int,
    filename: str,
    db: AsyncSession,
) -> None:
    """
    AI Ï±ÑÌåÖÏóê Í≤ÄÏ¶ù Î≥¥Í≥†ÏÑúÎ•º Ï†ÑÏÜ°
    """
    try:
        # Rails APIÎ•º ÌÜµÌï¥ AI Ï±ÑÌåÖÏóê Î©îÏãúÏßÄ Ï∂îÍ∞Ä
        import httpx

        rails_api_url = settings.RAILS_API_URL
        rails_api_key = settings.RAILS_INTERNAL_API_KEY

        async with httpx.AsyncClient() as client:
            # ChatMessage ÏÉùÏÑ± ÏöîÏ≤≠
            response = await client.post(
                f"{rails_api_url}/api/v1/ai_consultation/chat_messages",
                headers={
                    "Authorization": f"Bearer {rails_api_key}",
                    "Content-Type": "application/json",
                },
                json={
                    "session_id": session_id,
                    "user_id": user_id,
                    "role": "assistant",
                    "content": report,
                    "metadata": {
                        "type": "validation_report",
                        "filename": filename,
                        "error_count": error_count,
                        "generated_by": "excel_validator",
                    },
                },
            )

            if response.status_code != 200:
                logger.error(
                    f"Failed to send report to AI chat: {response.status_code} - {response.text}"
                )
            else:
                logger.info(
                    f"Successfully sent validation report to AI chat for session {session_id}"
                )

    except Exception as e:
        logger.error(f"Error sending report to AI chat: {str(e)}")
        # Don't raise exception - this is a non-critical failure


def _generate_fallback_report(
    context: Dict[str, Any], critical_errors: List[str], i18n: I18nContext
) -> str:
    """
    AI ÏÑúÎπÑÏä§ Ïã§Ìå® Ïãú Í∏∞Î≥∏ Î≥¥Í≥†ÏÑú ÏÉùÏÑ±
    """
    if i18n.language == "ko":
        return f"""
üìä Excel ÌååÏùº Í≤ÄÏ¶ù Í≤∞Í≥º

**ÌååÏùºÎ™Ö**: {context['filename']}

**Ï†ÑÏ≤¥ ÏöîÏïΩ**
Í≤ÄÏ¶ù Í≤∞Í≥º Ï¥ù {context['total_errors']}Í∞úÏùò Ïò§Î•òÍ∞Ä Î∞úÍ≤¨ÎêòÏóàÏäµÎãàÎã§.
Ïù¥ Ï§ë {context['critical_errors']}Í∞úÎäî Ï¶âÏãú ÏàòÏ†ïÏù¥ ÌïÑÏöîÌïú Ï§ëÏöî Ïò§Î•òÏù¥Î©∞,
{context['auto_fixable']}Í∞úÎäî ÏûêÎèôÏúºÎ°ú ÏàòÏ†ï Í∞ÄÎä•Ìï©ÎãàÎã§.

**Ï£ºÏöî Î∞úÍ≤¨ÏÇ¨Ìï≠**
{chr(10).join(critical_errors[:5])}

**Í∂åÏû• Ï°∞ÏπòÏÇ¨Ìï≠**
1. Ï§ëÏöî Ïò§Î•òÎ∂ÄÌÑ∞ Ïö∞ÏÑ†Ï†ÅÏúºÎ°ú ÏàòÏ†ïÌïòÏÑ∏Ïöî
2. ÏûêÎèô ÏàòÏ†ï Í∏∞Îä•ÏùÑ ÌôúÏö©ÌïòÏó¨ Îπ†Î•¥Í≤å Ìï¥Í≤∞ Í∞ÄÎä•Ìïú Ïò§Î•òÎ•º Ï≤òÎ¶¨ÌïòÏÑ∏Ïöî
3. ÏàòÏãù Ïò§Î•òÎäî Ï∞∏Ï°∞ Î≤îÏúÑÎ•º ÌôïÏù∏ÌïòÍ≥† ÏàòÏ†ïÌïòÏÑ∏Ïöî
4. Îç∞Ïù¥ÌÑ∞ Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨Î•º Ï∂îÍ∞ÄÌïòÏó¨ Ìñ•ÌõÑ Ïò§Î•òÎ•º Î∞©ÏßÄÌïòÏÑ∏Ïöî

**Îã§Ïùå Îã®Í≥Ñ**
- "ÏûêÎèô ÏàòÏ†ï" Î≤ÑÌäºÏùÑ ÌÅ¥Î¶≠ÌïòÏó¨ ÏàòÏ†ï Í∞ÄÎä•Ìïú Ïò§Î•òÎ•º Ìï¥Í≤∞ÌïòÏÑ∏Ïöî
- Í∞úÎ≥Ñ Ïò§Î•òÎ•º ÌÅ¥Î¶≠ÌïòÏó¨ ÏÉÅÏÑ∏ Ï†ïÎ≥¥ÏôÄ ÏàòÏ†ï Î∞©Î≤ïÏùÑ ÌôïÏù∏ÌïòÏÑ∏Ïöî
- ÏàòÏ†ï ÌõÑ Îã§Ïãú Í≤ÄÏ¶ùÌïòÏó¨ Î™®Îì† Ïò§Î•òÍ∞Ä Ìï¥Í≤∞ÎêòÏóàÎäîÏßÄ ÌôïÏù∏ÌïòÏÑ∏Ïöî
"""
    else:
        return f"""
üìä Excel File Validation Results

**Filename**: {context['filename']}

**Summary**
Validation found {context['total_errors']} total errors.
{context['critical_errors']} are critical errors requiring immediate attention,
and {context['auto_fixable']} can be automatically fixed.

**Key Findings**
{chr(10).join(critical_errors[:5])}

**Recommended Actions**
1. Priority fix critical errors first
2. Use auto-fix feature for quickly resolvable errors
3. Check and correct formula references
4. Add data validation to prevent future errors

**Next Steps**
- Click "Auto Fix" to resolve fixable errors
- Click individual errors for details and fix methods
- Re-validate after fixes to ensure all errors are resolved
"""


@router.post("/verify-cell-position")
async def verify_cell_position(
    request: CellVerificationRequest,
    db: AsyncSession = Depends(get_db),
    i18n: I18nContext = Depends(get_i18n_context),
) -> Dict[str, Any]:
    """
    Verify cell position by analyzing the same cell in Python
    Used for testing coordinate consistency between ExcelJS and Python
    """
    try:
        # TODO: Ïã§Ï†ú ÌååÏùº Í≤ΩÎ°úÎ•º file_idÎ°úÎ∂ÄÌÑ∞ Í∞ÄÏ†∏Ïò§Îäî Î°úÏßÅ Íµ¨ÌòÑ
        # ÌòÑÏû¨Îäî ÌÖåÏä§Ìä∏Ïö©ÏúºÎ°ú ÌïòÎìúÏΩîÎî©
        if request.file_id == "36":
            file_path = "/Users/kevin/Desktop/ÏÇ¨Í≥†Ï°∞ÏÇ¨.xlsx"
        else:
            # Í∏∞Î≥∏ ÌÖåÏä§Ìä∏ ÌååÏùº ÏÇ¨Ïö©
            file_path = "/Users/kevin/Desktop/ÏÇ¨Í≥†Ï°∞ÏÇ¨.xlsx"

        if not os.path.exists(file_path):
            raise HTTPException(status_code=404, detail=f"File not found: {file_path}")

        # openpyxlÎ°ú Excel ÌååÏùº ÏùΩÍ∏∞
        import openpyxl

        workbook = openpyxl.load_workbook(file_path, data_only=True)
        worksheet = workbook.active

        # ÏÖÄ Ï£ºÏÜåÎ•º Ï¢åÌëúÎ°ú Î≥ÄÌôò (Ïòà: "C3" -> row=3, col=3)
        try:
            cell = worksheet[request.cell_address]
            python_address = cell.coordinate
            python_value = cell.value
            python_row = cell.row
            python_col = cell.column

            # ÏÖÄ ÌÉÄÏûÖ ÌåêÎã®
            if python_value is None:
                cell_type = "empty"
            elif isinstance(python_value, (int, float)):
                cell_type = "number"
            elif isinstance(python_value, str):
                cell_type = "text"
            else:
                cell_type = "other"

            workbook.close()

            return {
                "success": True,
                "address": python_address,
                "value": python_value,
                "row": python_row,
                "column": python_col,
                "type": cell_type,
                "requested_address": request.cell_address,
                "match": request.cell_address == python_address,
            }

        except Exception as cell_error:
            workbook.close()
            return {
                "success": False,
                "error": f"Cell access error: {str(cell_error)}",
                "address": "ERROR",
                "value": None,
                "type": "error",
                "requested_address": request.cell_address,
                "match": False,
            }

    except Exception as e:
        logger.error(f"Cell position verification failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Verification failed: {str(e)}")
